{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48bc123f-62de-4c3e-83b7-be57532aeb48",
   "metadata": {},
   "source": [
    "# MoodSync: Emotion-Responsive Lighting System\n",
    "\n",
    "**MoodSync** is a unique project that changes the lighting in a room based on the detected emotion from a user's facial expression. The project uses a Convolutional Neural Network (CNN) to classify emotions from facial images and adjusts the brightness of Philips Hue lights accordingly.\n",
    "\n",
    "## Project Story\n",
    "\n",
    "The inspiration for MoodSync came from a simple moment. I was hanging out with my sister, and when I was leaving her room, she asked me to dim the lights because she was feeling down about a recent math test score. This got me thinkingâ€”what if there was a way for lights to adjust automatically based on emotions? MoodSync is the result of this thought, combining computer vision and IoT to create an emotion-responsive environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f341cc5-4c44-461b-a738-5c05487381d4",
   "metadata": {},
   "source": [
    "![FlowChart](FlowChart.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e5a025-3e6e-432f-963f-df367b06cb27",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "To run MoodSync, you need to install the following libraries:\n",
    "- **TensorFlow**: For building and training the CNN model.\n",
    "- **Flask**: For creating the web app that hosts the model.\n",
    "- **Requests**: To interact with the Philips Hue API.\n",
    "- **Pillow**: For image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b40a68-6734-4684-a6ff-c170f82d164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow flask requests pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a23157-9f68-4f14-8d3f-190ef62f5bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and Keras for building the CNN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Flask for the web app\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "\n",
    "# Requests for interacting with the Philips Hue API\n",
    "import requests\n",
    "\n",
    "# Pillow for image processing\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Additional utilities\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ad7829-20d1-4b3c-add1-6157df292ae8",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "For this project, we use a facial emotion dataset. The dataset is organized by emotions (e.g., Happy, Sad, Neutral), and each category contains images of faces expressing that emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce33159-2408-4d71-aac0-e5798488a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define paths for training and validation data\n",
    "training_dir = 'images/images/train'\n",
    "validation_dir = 'images/images/validation'\n",
    "\n",
    "# Image augmentation for training data\n",
    "training_data = ImageDataGenerator(\n",
    "    rescale=1./255, \n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2, \n",
    "    height_shift_range=0.2, \n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Only rescaling for validation data\n",
    "validation_data = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load images from directories\n",
    "training_generator = training_data.flow_from_directory(\n",
    "    training_dir, target_size=(64, 64), batch_size=64, class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_data.flow_from_directory(\n",
    "    validation_dir, target_size=(64, 64), batch_size=64, class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b8bd2e-8f4c-40a1-9f67-d57af3101945",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "MoodSync uses a Convolutional Neural Network (CNN) model to classify emotions based on facial images. The model architecture includes multiple convolutional and pooling layers to extract facial features, followed by dense layers for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3650d244-78f2-43b4-b5fb-0e9e0c5820e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Building the model\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(64,64,3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2,2), padding='same'))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "model.add(layers.Conv2D(256, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2,2), padding='same'))\n",
    "\n",
    "model.add(layers.Conv2D(512, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2,2), padding='same'))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b25d2f1-93c7-4af6-a035-a4e4ab69eabd",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "The model is trained for 50 epochs with early stopping to prevent overfitting. This helps the model to learn from the dataset effectively and stop training when improvements stop. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509ffcb-32b7-4528-80bf-528aa4fd6be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Set callbacks for early stopping and model checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('model_weights.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    training_generator, \n",
    "    epochs=50,\n",
    "    validation_data=validation_generator, \n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d3714b-e257-475b-b914-25ef1d7d38f1",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "After training, we evaluate the model's performance on the validation dataset to see how well it generalizes to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1200a2ad-b5fe-4e4d-8f92-ef5513b45153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "validation_loss, validation_accuracy = model.evaluate(validation_generator)\n",
    "print(f\"Validation Accuracy: {validation_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c57d61a-8677-46c8-b278-dda0baa97018",
   "metadata": {},
   "source": [
    "## Flask Application\n",
    "\n",
    "The Flask application serves as the interface for MoodSync. Users upload images, and the application predicts their emotion using the CNN model. Based on the predicted emotion, the Philips Hue lights are adjusted to match the user's mood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e9f749-afdc-4da3-ac6f-178bccfed9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, jsonify\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the CNN model\n",
    "model = tf.keras.models.load_model('best_model.keras')\n",
    "\n",
    "# Preprocess image function\n",
    "def preprocess_image(image):\n",
    "    try:\n",
    "        img = image.resize((64, 64))  # Resize the image to 64x64 (as expected by the model)\n",
    "        img_array = np.array(img)  # Convert to numpy array\n",
    "        print(f\"Image converted to array: {img_array.shape}\")  # Check the shape of the array\n",
    "        img_array = img_array / 255.0  # Normalize the image\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error during image preprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "# Brightness levels for each emotion\n",
    "EMOTION_BRIGHTNESS = {\n",
    "    'Happy': 90,\n",
    "    'Sad': 20,\n",
    "    'Neutral': 50\n",
    "    # Add more mappings if you have additional emotions\n",
    "}\n",
    "\n",
    "# Philips Hue API configuration\n",
    "HUE_BRIDGE_IP = '192.168.2.159'  # Replace with your Philips Hue bridge IP\n",
    "LIGHT_ID = '1'  # Your light ID in the bridge\n",
    "API_KEY = '0uwbM4jTAojbncWxyrQZJxiBtDKvlx23GuSjZhQ1'  # Your Philips Hue API key\n",
    "\n",
    "def set_brightness(brightness):\n",
    "    url = f\"http://{HUE_BRIDGE_IP}/api/{API_KEY}/lights/{LIGHT_ID}/state\"\n",
    "    payload = {\"bri\": int((brightness / 100) * 254), \"on\": True}  # Convert percentage to Hue brightness scale (0-254)\n",
    "    try:\n",
    "        print(f\"Setting brightness to {brightness}%...\")\n",
    "        response = requests.put(url, json=payload)\n",
    "        print(f\"Sending brightness adjustment request to {url} with payload {payload}\")\n",
    "        print(f\"Response status code: {response.status_code}\")\n",
    "        print(f\"Response content: {response.content}\")\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Brightness successfully set to {brightness}%\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Failed to adjust brightness.\")\n",
    "            return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error adjusting light brightness: {e}\")\n",
    "        return False\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/upload_image', methods=['POST'])\n",
    "def upload_image():\n",
    "    try:\n",
    "        # Check if image is present in the request\n",
    "        if 'image' not in request.files:\n",
    "            print(\"No image provided\")\n",
    "            return jsonify({'error': 'No image provided'}), 400\n",
    "        \n",
    "        # Read the image from the request\n",
    "        file = request.files['image']\n",
    "        img = Image.open(io.BytesIO(file.read()))\n",
    "        print(f\"Image received and loaded with size: {img.size}\")\n",
    "\n",
    "        # Preprocess the image for the CNN model\n",
    "        img_array = preprocess_image(img)\n",
    "        print(f\"Preprocessed image shape: {img_array.shape}\")\n",
    "\n",
    "        # Get prediction from the model\n",
    "        predictions = model.predict(img_array)\n",
    "        print(f\"Model predictions: {predictions}\")\n",
    "        \n",
    "        predicted_class = np.argmax(predictions, axis=1)\n",
    "        print(f\"Predicted class index: {predicted_class}\")\n",
    "\n",
    "        # Map the class to emotion labels\n",
    "        emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "        predicted_emotion = emotion_labels[predicted_class[0]]\n",
    "        print(f\"Predicted emotion: {predicted_emotion}\")\n",
    "\n",
    "        # Get brightness for emotion\n",
    "        brightness = EMOTION_BRIGHTNESS.get(predicted_emotion, 50)  # Default to 50% if emotion not found\n",
    "        print(f\"Your emotion was predicted as '{predicted_emotion}'. Adjusting Philips Hue light to {brightness}% brightness.\")\n",
    "\n",
    "        # Set the brightness through Philips Hue API\n",
    "        if set_brightness(brightness):\n",
    "            print(f\"Philips Hue light adjusted to match '{predicted_emotion}' emotion with {brightness}% brightness.\")\n",
    "            return jsonify({'predicted_emotion': predicted_emotion, 'brightness': brightness})\n",
    "        else:\n",
    "            print(\"Failed to adjust Philips Hue light brightness.\")\n",
    "            return jsonify({'predicted_emotion': predicted_emotion, 'error': 'Failed to adjust light brightness'}), 500\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        return jsonify({'error': f\"Error processing image: {str(e)}\"}), 500\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be773c9d-26b3-45db-8cee-528f9baac29b",
   "metadata": {},
   "source": [
    "## Frontend Code (HTML, CSS, JavaScript)\n",
    "\n",
    "The frontend includes a simple interface where users can capture an image, submit it to the Flask backend, and view the detected emotion. Below is the HTML, CSS, and JavaScript code for the interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eca73c-4c74-48de-85ac-e98e2bef2d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>MoodSync</title>\n",
    "    <!-- Link to the styles.css using Flask's url_for function -->\n",
    "    <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='styles.css') }}\">\n",
    "</head>\n",
    "<body>\n",
    "    <div id=\"welcome-screen\">\n",
    "        <h1 id=\"welcome-title\">MoodSync</h1>\n",
    "        <h3 id=\"welcome-caption\" class=\"fade-caption\">Adjust your lights based on your current emotion</h3>\n",
    "    </div>\n",
    "    \n",
    "    <div id=\"main-interface\" class=\"hidden\">\n",
    "        <div class=\"box\">\n",
    "            <h1>MoodSync</h1>\n",
    "            <p>Adjust your lights based on your current emotion</p>\n",
    "\n",
    "            <!-- Capture Emotion -->\n",
    "            <button id=\"capture-button\" class=\"capture-btn\">Capture Emotion</button>\n",
    "\n",
    "\n",
    "            <p id=\"emotion-status\">Your Current Emotion is: <span id=\"emotion-result\">None</span></p>\n",
    "            \n",
    "            <!-- Video Stream and Image Display -->\n",
    "            <div id=\"video-container\" class=\"hidden\">\n",
    "                <video id=\"camera-stream\" autoplay></video>\n",
    "            </div>\n",
    "            <img id=\"selected-image\" src=\"\" alt=\"Captured Image\" class=\"hidden\" />\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <!-- Link to the app.js using Flask's url_for function -->\n",
    "    <script src=\"{{ url_for('static', filename='app.js') }}\"></script>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa14cc7c-220e-423c-93bd-7e329812d8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "/* General Styles */\n",
    "body {\n",
    "    font-family: 'Arial', sans-serif;\n",
    "    margin: 0;\n",
    "    padding: 0;\n",
    "    background-color: #f0f0f0;\n",
    "    display: flex;\n",
    "    justify-content: center;\n",
    "    align-items: center;\n",
    "    flex-direction: column;\n",
    "    height: 100vh;\n",
    "}\n",
    "\n",
    "h1, h3 {\n",
    "    margin: 0;\n",
    "}\n",
    "\n",
    ".hidden {\n",
    "    display: none;\n",
    "}\n",
    "\n",
    "/* Welcome Screen */\n",
    "#welcome-screen {\n",
    "    text-align: center;\n",
    "    font-size: 2.5rem;\n",
    "}\n",
    "\n",
    "#welcome-title {\n",
    "    font-size: 4rem;\n",
    "    font-weight: bold;\n",
    "    color: #333;\n",
    "    animation: fadeInOut 2s ease-in-out infinite; /* Independent fade animation for MoodSync title */\n",
    "}\n",
    "\n",
    "#welcome-caption {\n",
    "    margin-top: 20px;\n",
    "    font-size: 1.5rem;\n",
    "    color: #666;\n",
    "}\n",
    "\n",
    "/* Keyframes for Letter Fade-out Animation */\n",
    "@keyframes fadeOutLetter {\n",
    "    0% { opacity: 1; }\n",
    "    100% { opacity: 0; }\n",
    "}\n",
    "\n",
    "/* Keyframes for the Title Fade-in, Fade-out */\n",
    "@keyframes fadeInOut {\n",
    "    0%, 100% { opacity: 1; }\n",
    "    50% { opacity: 0; }\n",
    "}\n",
    "\n",
    "/* Main Interface */\n",
    "#main-interface {\n",
    "    text-align: center;\n",
    "    margin-top: 30px;\n",
    "}\n",
    "\n",
    ".box {\n",
    "    background-color: white;\n",
    "    padding: 40px;\n",
    "    border-radius: 15px;\n",
    "    box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);\n",
    "    width: 500px;\n",
    "}\n",
    "\n",
    "h1 {\n",
    "    font-size: 3rem;\n",
    "    color: #333;\n",
    "    margin-bottom: 20px;\n",
    "}\n",
    "\n",
    "p {\n",
    "    font-size: 1.2rem;\n",
    "    color: #555;\n",
    "}\n",
    "\n",
    ".capture-btn, .view-backend-btn {\n",
    "    font-size: 1.2rem;\n",
    "    padding: 10px 20px;\n",
    "    margin: 10px;\n",
    "    border: none;\n",
    "    border-radius: 5px;\n",
    "    cursor: pointer;\n",
    "    background-color: #2196f3;\n",
    "    color: white;\n",
    "}\n",
    "\n",
    "#emotion-result {\n",
    "    color: red;\n",
    "    font-weight: bold;\n",
    "}\n",
    "\n",
    "#selected-image, #camera-stream {\n",
    "    margin-top: 20px;\n",
    "    max-width: 100%;\n",
    "    height: auto;\n",
    "    border-radius: 15px;\n",
    "    border: 3px solid #ccc;\n",
    "}\n",
    "\n",
    "#camera-stream {\n",
    "    width: 80%; \n",
    "    max-width: 400px;\n",
    "    display: block;\n",
    "    margin: 0 auto;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf547f30-71cd-4562-a005-040c2074fceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "document.addEventListener('DOMContentLoaded', function () {\n",
    "    const welcomeScreen = document.getElementById('welcome-screen');\n",
    "    const welcomeCaption = document.getElementById('welcome-caption');\n",
    "    const mainInterface = document.getElementById('main-interface');\n",
    "\n",
    "    // Wrap each letter of the caption in span tags for animation\n",
    "    welcomeCaption.innerHTML = welcomeCaption.textContent.split(\"\").map(letter => `<span>${letter}</span>`).join(\"\");\n",
    "\n",
    "    // Show the main interface after the caption finishes fading out\n",
    "    welcomeScreen.addEventListener('click', function () {\n",
    "        const letterSpans = document.querySelectorAll('.fade-caption span');\n",
    "        \n",
    "        // Faster letter fade-out (reduce duration)\n",
    "        letterSpans.forEach((span, index) => {\n",
    "            span.style.animation = `fadeOutLetter 0.3s ease-in-out forwards`; // Faster fade-out\n",
    "            span.style.animationDelay = `${index * 0.05}s`; // Shorter delay between each letter\n",
    "        });\n",
    "\n",
    "        // Wait for the last letter to finish animating before switching screens\n",
    "        setTimeout(() => {\n",
    "            welcomeScreen.classList.add('hidden');\n",
    "            mainInterface.classList.remove('hidden');\n",
    "        }, letterSpans.length * 50 + 500); // Adjust timing based on faster fade-out\n",
    "    });\n",
    "\n",
    "    const captureButton = document.getElementById('capture-button');\n",
    "    const backendButton = document.getElementById('view-backend');\n",
    "    const emotionResult = document.getElementById('emotion-result');\n",
    "    const selectedImage = document.getElementById('selected-image');\n",
    "    const videoContainer = document.getElementById('video-container');\n",
    "    const videoStreamElement = document.getElementById('camera-stream');\n",
    "\n",
    "    let isCameraOpen = false;\n",
    "    let videoStream = null;\n",
    "\n",
    "    function resetUI() {\n",
    "        // Hide captured image and reset UI\n",
    "        selectedImage.classList.add('hidden');\n",
    "        videoContainer.classList.add('hidden');\n",
    "        emotionResult.innerText = 'None';\n",
    "        selectedImage.src = '';\n",
    "    }\n",
    "\n",
    "    captureButton.addEventListener('click', function () {\n",
    "        if (!isCameraOpen) {\n",
    "            resetUI();\n",
    "    \n",
    "            // Open the user's camera\n",
    "            navigator.mediaDevices.getUserMedia({ video: true })\n",
    "                .then(function (stream) {\n",
    "                    videoStream = stream;\n",
    "                    videoStreamElement.srcObject = stream;\n",
    "                    videoContainer.classList.remove('hidden');\n",
    "                    captureButton.innerText = 'Take a Snapshot';\n",
    "                    isCameraOpen = true;\n",
    "                })\n",
    "                .catch(function (error) {\n",
    "                    alert(\"Unable to access camera. Please allow camera permissions.\");\n",
    "                });\n",
    "        } else {\n",
    "            // Take a snapshot\n",
    "            const canvas = document.createElement('canvas');\n",
    "            canvas.width = videoStreamElement.videoWidth;\n",
    "            canvas.height = videoStreamElement.videoHeight;\n",
    "            const ctx = canvas.getContext('2d');\n",
    "            ctx.drawImage(videoStreamElement, 0, 0, canvas.width, canvas.height);\n",
    "    \n",
    "            // Convert the canvas image to a blob\n",
    "            canvas.toBlob(function (blob) {\n",
    "                const formData = new FormData();\n",
    "                formData.append('image', blob, 'snapshot.jpg'); // Append the image as 'snapshot.jpg'\n",
    "    \n",
    "                // Send the image to the backend for processing\n",
    "                fetch('/upload_image', {\n",
    "                    method: 'POST',\n",
    "                    body: formData\n",
    "                })\n",
    "                .then(response => response.json())\n",
    "                .then(data => {\n",
    "                    // Update the UI with the predicted emotion\n",
    "                    emotionResult.innerText = data.predicted_emotion;\n",
    "                    selectedImage.src = URL.createObjectURL(blob);  // Show the snapshot image\n",
    "                    selectedImage.classList.remove('hidden');\n",
    "                    videoContainer.classList.add('hidden');\n",
    "                })\n",
    "                .catch(error => {\n",
    "                    console.error('Error:', error);\n",
    "                    alert('Error processing the image.');\n",
    "                });\n",
    "            }, 'image/jpeg');\n",
    "    \n",
    "            // Stop the video stream\n",
    "            videoStream.getTracks().forEach(track => track.stop());\n",
    "    \n",
    "            captureButton.innerText = 'Capture Emotion';\n",
    "            isCameraOpen = false;\n",
    "        }\n",
    "    });\n",
    "\n",
    "    \n",
    "});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1eaef8-dcb2-4569-9d9a-d5078e19d4b6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "MoodSync is an innovative project that demonstrates the integration of computer vision with IoT to create an emotion-responsive environment. While this is just a proof-of-concept, it shows how technology can be used to create a more personalized and responsive atmosphere.\n",
    "\n",
    "### Future Improvements\n",
    "- Extend the range of detectable emotions.\n",
    "- Integrate with additional smart home systems.\n",
    "- Add options for different lighting effects.\n",
    "\n",
    "Thank you for exploring MoodSync!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f060f1-28e1-4920-a40f-a34649c8ddb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
