{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48bc123f-62de-4c3e-83b7-be57532aeb48",
   "metadata": {},
   "source": [
    "# MoodSync: Emotion-Responsive Lighting System\n",
    "\n",
    "**MoodSync** is a unique project that changes the lighting in a room based on the detected emotion from a user's facial expression. The project uses a Convolutional Neural Network (CNN) to classify emotions from facial images and adjusts the brightness of Philips Hue lights accordingly.\n",
    "\n",
    "## Project Story\n",
    "\n",
    "The inspiration for MoodSync came from a simple moment. I was hanging out with my sister, and when I was leaving her room, she asked me to dim the lights because she was feeling down about a recent math test score. This got me thinkingâ€”what if there was a way for lights to adjust automatically based on emotions? MoodSync is the result of this thought, combining computer vision and IoT to create an emotion-responsive environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f341cc5-4c44-461b-a738-5c05487381d4",
   "metadata": {},
   "source": [
    "![FlowChart](FlowChart.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e5a025-3e6e-432f-963f-df367b06cb27",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "To run MoodSync, you need to install the following libraries:\n",
    "- **TensorFlow**: For building and training the CNN model.\n",
    "- **Flask**: For creating the web app that hosts the model.\n",
    "- **Requests**: To interact with the Philips Hue API.\n",
    "- **Pillow**: For image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b40a68-6734-4684-a6ff-c170f82d164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow flask requests pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a23157-9f68-4f14-8d3f-190ef62f5bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and Keras for building the CNN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Flask for the web app\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "\n",
    "# Requests for interacting with the Philips Hue API\n",
    "import requests\n",
    "\n",
    "# Pillow for image processing\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Additional utilities\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ad7829-20d1-4b3c-add1-6157df292ae8",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "For this project, we use a facial emotion dataset. The dataset is organized by emotions (e.g., Happy, Sad, Neutral), and each category contains images of faces expressing that emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce33159-2408-4d71-aac0-e5798488a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define paths for training and validation data\n",
    "training_dir = 'images/images/train'\n",
    "validation_dir = 'images/images/validation'\n",
    "\n",
    "# Image augmentation for training data\n",
    "training_data = ImageDataGenerator(\n",
    "    rescale=1./255, \n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2, \n",
    "    height_shift_range=0.2, \n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Only rescaling for validation data\n",
    "validation_data = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load images from directories\n",
    "training_generator = training_data.flow_from_directory(\n",
    "    training_dir, target_size=(64, 64), batch_size=64, class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_data.flow_from_directory(\n",
    "    validation_dir, target_size=(64, 64), batch_size=64, class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b8bd2e-8f4c-40a1-9f67-d57af3101945",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "MoodSync uses a Convolutional Neural Network (CNN) model to classify emotions based on facial images. The model architecture includes multiple convolutional and pooling layers to extract facial features, followed by dense layers for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3650d244-78f2-43b4-b5fb-0e9e0c5820e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Building the model\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(64,64,3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2,2), padding='same'))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "model.add(layers.Conv2D(256, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2,2), padding='same'))\n",
    "\n",
    "model.add(layers.Conv2D(512, (3,3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2,2), padding='same'))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b25d2f1-93c7-4af6-a035-a4e4ab69eabd",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "The model is trained for 50 epochs with early stopping to prevent overfitting. This helps the model to learn from the dataset effectively and stop training when improvements stop. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509ffcb-32b7-4528-80bf-528aa4fd6be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Set callbacks for early stopping and model checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('model_weights.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    training_generator, \n",
    "    epochs=50,\n",
    "    validation_data=validation_generator, \n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d3714b-e257-475b-b914-25ef1d7d38f1",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "After training, we evaluate the model's performance on the validation dataset to see how well it generalizes to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1200a2ad-b5fe-4e4d-8f92-ef5513b45153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "validation_loss, validation_accuracy = model.evaluate(validation_generator)\n",
    "print(f\"Validation Accuracy: {validation_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c57d61a-8677-46c8-b278-dda0baa97018",
   "metadata": {},
   "source": [
    "## Flask Application\n",
    "\n",
    "The Flask application serves as the interface for MoodSync. Users upload images, and the application predicts their emotion using the CNN model. Based on the predicted emotion, the Philips Hue lights are adjusted to match the user's mood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be773c9d-26b3-45db-8cee-528f9baac29b",
   "metadata": {},
   "source": [
    "## Frontend Code (HTML, CSS, JavaScript)\n",
    "\n",
    "The frontend includes a simple interface where users can capture an image, submit it to the Flask backend, and view the detected emotion. Below is the HTML, CSS, and JavaScript code for the interface.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1eaef8-dcb2-4569-9d9a-d5078e19d4b6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "MoodSync is an innovative project that demonstrates the integration of computer vision with IoT to create an emotion-responsive environment. While this is just a proof-of-concept, it shows how technology can be used to create a more personalized and responsive atmosphere.\n",
    "\n",
    "### Future Improvements\n",
    "- Extend the range of detectable emotions.\n",
    "- Integrate with additional smart home systems.\n",
    "- Add options for different lighting effects.\n",
    "\n",
    "Thank you for exploring MoodSync!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
